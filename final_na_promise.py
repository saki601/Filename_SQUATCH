# -*- coding: utf-8 -*-
"""Final na promise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lGbyF1T1gBmUr7Qfjlt8PER4RyU21Z80

#Dataset
"""

!pip install --upgrade datasets

from datasets import load_dataset
dataset = load_dataset("midas/inspec", "raw")

train_sample = dataset["train"][0]
print("Fields in the sample: ", [key for key in train_sample.keys()])
print("Tokenized Document: ", train_sample["document"])
print("Document BIO Tags: ", train_sample["doc_bio_tags"])
print("Extractive/present Keyphrases: ", train_sample["extractive_keyphrases"])
print("Abstractive/absent Keyphrases: ", train_sample["abstractive_keyphrases"])
print("\n-----------\n")

# sample from the validation split
print("Sample from validation dataset split")
validation_sample = dataset["validation"][0]
print("Fields in the sample: ", [key for key in validation_sample.keys()])
print("Tokenized Document: ", validation_sample["document"])
print("Document BIO Tags: ", validation_sample["doc_bio_tags"])
print("Extractive/present Keyphrases: ", validation_sample["extractive_keyphrases"])
print("Abstractive/absent Keyphrases: ", validation_sample["abstractive_keyphrases"])
print("\n-----------\n")

# sample from the test split
print("Sample from test dataset split")
test_sample = dataset["test"][0]
print("Fields in the sample: ", [key for key in test_sample.keys()])
print("Tokenized Document: ", test_sample["document"])
print("Document BIO Tags: ", test_sample["doc_bio_tags"])
print("Extractive/present Keyphrases: ", test_sample["extractive_keyphrases"])
print("Abstractive/absent Keyphrases: ", test_sample["abstractive_keyphrases"])
print("\n-----------\n")

"""# Before Fine tuning"""

!pip install --upgrade evaluate rouge_score

from datasets import load_dataset
import evaluate
from transformers import PegasusForConditionalGeneration, PegasusTokenizer
import torch
!pip install evaluate rouge_score
from evaluate import load
import numpy as np

rouge = evaluate.load("rouge")

# Load Pegasus model and tokenizer
model_name = "google/pegasus-xsum"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)

def generate_texts(documents, prefix="", max_length=60, num_beams=5):
    generated_texts = []
    for doc_tokens in documents:
        # Join tokens into a string
        doc_text = " ".join(doc_tokens) if isinstance(doc_tokens, list) else doc_tokens
        input_text = prefix + doc_text

        inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding="longest").to(device)
        outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_texts.append(decoded)
    return generated_texts

# Prepare documents and references for testing
test_documents = dataset['test']['document']  # list of token lists
reference_keyphrases = dataset['test']['abstractive_keyphrases']  # list of lists of phrases

# Join reference keyphrases as a single string (ROUGE expects string references)
reference_keyphrases_joined = [" ".join(kps) for kps in reference_keyphrases]

# Generate keyphrases with prefix
generated_keyphrases = generate_texts(test_documents, prefix="keyphrases: ")

# Generate summaries without prefix
generated_summaries = generate_texts(test_documents, prefix=":")

print("Sample document tokens:")
print(test_documents[0])

print("\nGenerated Summary:")
print(generated_summaries[0])

print("\nGenerated Keyphrases:")
print(generated_keyphrases[0])

print("\nReference Keyphrases:")
print(reference_keyphrases_joined[0])

# Compute ROUGE for keyphrases
rouge_scores = rouge.compute(predictions=generated_keyphrases, references=reference_keyphrases_joined)

print("\nROUGE Scores for Keyphrases:")
for k, v in rouge_scores.items():
    print(f"{k}: {v:.4f}")

"""# Fine tuned"""

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

model_name = "google/pegasus-large"
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)

from transformers import AutoTokenizer

def tokenize_function(examples):
    # Join the token list into plain string
    inputs = [" ".join(doc) for doc in examples["document"]]
    # Join target keyphrases into a single string (separated by '; ')
    targets = ["; ".join(keys) for keys in examples["abstractive_keyphrases"]]

    # Tokenize the inputs and targets
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=64, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize all splits
tokenized_datasets = dataset.map(tokenize_function, batched=True)

from peft import get_peft_model, LoraConfig, TaskType

# Create LoRA configuration
lora_config = LoraConfig(
    r=8,                         # LoRA rank
    lora_alpha=32,               # LoRA scaling factor
    target_modules=["q_proj", "v_proj"]  , # LoRA target modules; common for Pegasus
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM  # For Pegasus-style encoder-decoder
)

# Wrap model with LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)

scaler = torch.cuda.amp.GradScaler(enabled=True)

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

# Define training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",  # Output directory for logs and checkpoints
    eval_strategy="epoch",
    # learning_rate=1e-4,
    # Reduce batch size
    per_device_train_batch_size=4, # Reduced from 16
    per_device_eval_batch_size=4,
    num_train_epochs=16,
    weight_decay=0.01,
    save_total_limit=3,
    predict_with_generate=True,
    fp16=True,  # Set to True if your GPU supports FP16 (recommended for memory saving)
    # Add gradient accumulation steps
    gradient_accumulation_steps=8, # Accumulate gradients over 2 steps
    logging_steps=100,
    #lr_scheduler_type='linear', # Learning rate scheduler
    #warmup_steps=100,
)

# Initialize the Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    optimizers=(optimizer, None),
)

# Continue with trainer.train()

trainer.train()

trainer.save_model("./results/final_checkpoint")

!ls ./results

# Save the fine-tuned model with the PEFT adapter
trainer.save_model("./results/final_checkpoint")

from peft import PeftModel
# Load the base model
base_model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-large")

# Load the LoRA adapter on top of the base model
finetuned_model = PeftModel.from_pretrained(base_model, "./results/final_checkpoint")

# Load the tokenizer
finetuned_tokenizer = PegasusTokenizer.from_pretrained("google/pegasus-large")

# Move the model to the appropriate device
device = "cuda" if torch.cuda.is_available() else "cpu"
finetuned_model.to(device)

def generate_texts_finetuned(documents, prefix="", max_length=60, num_beams=5):
    generated_texts = []
    for doc_tokens in documents:
        # Join tokens into a string
        doc_text = " ".join(doc_tokens) if isinstance(doc_tokens, list) else doc_tokens
        input_text = prefix + doc_text

        inputs = finetuned_tokenizer(input_text, return_tensors="pt", truncation=True, padding="longest").to(device)
        outputs = finetuned_model.generate(**inputs, max_length=max_length, num_beams=num_beams)
        decoded = finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_texts.append(decoded)
    return generated_texts

generated_keyphrases_finetuned = generate_texts_finetuned(test_documents, prefix="keyphrases: ")


generated_summaries_finetuned = generate_texts_finetuned(test_documents, prefix=":") # Using ":" based on your original code

rouge_scores_keyphrases_finetuned = rouge.compute(predictions=generated_keyphrases_finetuned, references=reference_keyphrases_joined)

print("\nROUGE Scores for Fine-tuned Keyphrases:")
for k, v in rouge_scores_keyphrases_finetuned.items():
    print(f"{k}: {v:.4f}")

print("\nSample Generated Summary (Fine-tuned):")
print(generated_summaries_finetuned[0])
print("\nGenerated Keyphrases:")
print(generated_keyphrases_finetuned[0])
